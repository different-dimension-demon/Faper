{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_embedding.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    # 假设每行数据由空格分隔\n",
    "    values = line.strip().split()\n",
    "    # 将字符串转换为浮点数\n",
    "    values = [float(val) for val in values]\n",
    "    data.append(values)\n",
    "\n",
    "# 将数据转换为张量\n",
    "train_embedding = data\n",
    "file.close()\n",
    "\n",
    "with open('train_result.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    # 假设每行数据由空格分隔\n",
    "    values = line.strip().split()\n",
    "    # 将字符串转换为浮点数\n",
    "    values = [float(val) for val in values]\n",
    "    data.append(values)\n",
    "\n",
    "# 将数据转换为张量\n",
    "train_result = data\n",
    "file.close()\n",
    "\n",
    "with open('test_embedding.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    # 假设每行数据由空格分隔\n",
    "    values = line.strip().split()\n",
    "    # 将字符串转换为浮点数\n",
    "    values = [float(val) for val in values]\n",
    "    data.append(values)\n",
    "\n",
    "# 将数据转换为张量\n",
    "test_embedding = data\n",
    "file.close()\n",
    "\n",
    "with open('test_result.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    # 假设每行数据由空格分隔\n",
    "    values = line.strip().split()\n",
    "    # 将字符串转换为浮点数\n",
    "    values = [float(val) for val in values]\n",
    "    data.append(values)\n",
    "\n",
    "# 将数据转换为张量\n",
    "test_result = data\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = np.asarray(train_embedding), np.asarray(train_result)\n",
    "X_test, Y_test = np.asarray(test_embedding), np.asarray(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch import kernels, means, models, mlls, settings\n",
    "from gpytorch import distributions as distr\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "train_embedding = torch.tensor(np.load('train_embedding.npy'))\n",
    "train_result = torch.tensor(np.log(np.load('train_result.npy')))\n",
    "test_embedding = torch.tensor(np.load('test_embedding.npy'))\n",
    "test_result = torch.tensor(np.log(np.load('test_result.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = means.ConstantMean()\n",
    "        self.covar_module = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return distr.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_embedding, train_result, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_max_label = 22.956885487470185\n",
    "norm_min_label = 0.0\n",
    "def unnormalize_torch(vals):\n",
    "    vals = (vals * (norm_max_label - norm_min_label)) + norm_min_label\n",
    "    return torch.exp(vals)\n",
    "\n",
    "def loss_function(predict, label):\n",
    "    qerror = []\n",
    "    predict = unnormalize_torch(predict)\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        if (predict[i] > label[i]):\n",
    "            qerror.append(predict[i] / label[i])\n",
    "        else:\n",
    "            qerror.append(label[i] / predict[i])\n",
    "\n",
    "    return torch.mean(torch.cat(qerror))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# mll = mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "mll = mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_embedding)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_result)\n",
    "    print(loss.shape)\n",
    "    loss_value = loss.item()  # 获取损失值\n",
    "\n",
    "    loss_value.backward()  # 计算梯度\n",
    "    # loss.backward()\n",
    "    if i % 5 == 4:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, FileType, ArgumentDefaultsHelpFormatter\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from jax import grad\n",
    "from functools import partial\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "# from jax.api import jit\n",
    "import jax.random as rand\n",
    "from jax import vmap\n",
    "from jax.lib import xla_bridge\n",
    "# from jax.config import config\n",
    "import jax.scipy as scipy\n",
    "from neural_tangents import stax\n",
    "import neural_tangents as nt\n",
    "# import datasets\n",
    "# import schemas\n",
    "from util import draw_uncertainty, calibration_plot, PredictionStatistics, draw_kernel_heatmap, show_memory_usage\n",
    "from util import uneven_train_test_split, train_test_val_split\n",
    "import torch\n",
    "\n",
    "pred_stat = PredictionStatistics()\n",
    "\n",
    "norm_max_label = 22.956885487470185\n",
    "norm_min_label = 0.0\n",
    "\n",
    "train_embedding = np.load('train_embedding.npy')\n",
    "train_result = np.log(np.load('train_result.npy'))/(norm_max_label-norm_min_label)\n",
    "train_result_real = np.load('train_result.npy')\n",
    "test_embedding = np.load('test_embedding.npy')\n",
    "test_result = np.log(np.load('test_result.npy'))/(norm_max_label-norm_min_label)\n",
    "test_result_real = np.load('test_result.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embedding_last_1000 = train_embedding[-396:]\n",
    "# test_embedding = np.concatenate([test_embedding, train_embedding_last_1000], axis=0)\n",
    "# train_embedding = train_embedding[:-396]\n",
    "\n",
    "# train_result_last_1000 = train_result[-396:]\n",
    "# test_result = np.concatenate([test_result, train_result_last_1000], axis=0)\n",
    "# train_result = train_result[:-396]\n",
    "\n",
    "# train_result_real_last_1000 = train_result_real[-396:]\n",
    "# test_result_real = np.concatenate([test_result_real, train_result_real_last_1000], axis=0)\n",
    "# train_result_real = train_result_real[:-396]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embedding.shape)\n",
    "print(train_result.shape)\n",
    "print(train_result_real.shape)\n",
    "print(test_embedding.shape)\n",
    "print(test_result.shape)\n",
    "print(test_result_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_torch(vals):\n",
    "    vals = (vals * (norm_max_label - norm_min_label)) + norm_min_label\n",
    "    return np.exp(vals)\n",
    "\n",
    "def loss_function(predict, label):\n",
    "    qerror = []\n",
    "    predict = unnormalize_torch(predict)\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        # print(predict[i], label[i])\n",
    "\n",
    "        if (predict[i] > label[i]):\n",
    "            qerror.append(predict[i] / label[i])\n",
    "        else:\n",
    "            qerror.append(label[i] / predict[i])\n",
    "    \n",
    "    return np.mean(np.concatenate(qerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNGP_train_and_test(X_train, Y_train, Y_train_real, X_test, Y_test, Y_test_real, query_infos_train = None, query_infos_test= None):\n",
    "\n",
    "\tdef prediction(pred_fn, X_test, kernel_type=\"nngp\", compute_cov = True):\n",
    "\t\tpred_mean, pred_cov = pred_fn(x_test=X_test, get=kernel_type, compute_cov= compute_cov)\n",
    "\t\treturn pred_mean, pred_cov\n",
    "\n",
    "\tinit_fn, apply_fn, kernel_fn = stax.serial(\n",
    "\t\tstax.Dense(1024), stax.Relu(), \n",
    "\t\tstax.Dense(1)\n",
    "\t)\n",
    "\tkernel_fn = nt.batch(kernel_fn, device_count = 0, batch_size = 0)\n",
    "\t# predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, X_test, Y_test, diag_reg = 1e-2)\n",
    "\tpredict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, np.concatenate([X_train, X_test], axis=0), np.concatenate([Y_train, Y_test], axis=0), diag_reg = 1e-1/7)\n",
    "\t\n",
    "\n",
    "\t# 验证训练集训练结果\n",
    "\tpred_mean_train, pred_cov_train = prediction(predict_fn, X_train, kernel_type = 'nngp')\n",
    "\tpred_std_train = np.sqrt(np.diag(pred_cov_train))\n",
    "\tq_error_train = loss_function(pred_mean_train, Y_train_real)\n",
    "\tprint(\"q_error_train: {}\".format(q_error_train))\n",
    "\n",
    "\t# 验证测试集\n",
    "\tpred_mean_test, pred_cov_test = prediction(predict_fn, X_test, kernel_type = 'nngp')\n",
    "\tpred_std_test = np.sqrt(np.diag(pred_cov_test))\n",
    "\tq_error_test = loss_function(pred_mean_test, Y_test_real)\n",
    "\tprint(\"q_error_test: {}\".format(q_error_test))\n",
    "\t\n",
    "\t# mse = np.sum(np.power(pred_mean -Y_test, 2))\n",
    "\t# #print(mse)\n",
    "\t# print(\"Mean Square Error: {}\".format(mse))\n",
    "\n",
    "\t#Obtain the inference time\n",
    "\t# print(X_test.shape, Y_test.shape)\n",
    "\t# start = datetime.datetime.now()\n",
    "\t# pred_mean, pred_cov = prediction(predict_fn, X_test, kernel_type = 'nngp')\n",
    "\t# end = datetime.datetime.now()\n",
    "\t# duration = (end - start).total_seconds()\n",
    "\t# print(\"Inference time={} seconds\".format(duration))\n",
    "\n",
    "\n",
    "\t# errors = onp.ravel(onp.array(pred_mean - Y_test))\n",
    "\t# pred_std = onp.ravel(onp.array(pred_std))\n",
    "\t# outputs = onp.ravel(onp.array(pred_mean))\n",
    "\t# Y_test = onp.ravel(onp.array(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNGP_train_and_test(train_embedding, train_result, train_result_real, test_embedding, test_result, test_result_real, None, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
